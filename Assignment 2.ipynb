{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: DenseNet with CIFAR10 Dataset by TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are required to implement DenseNet to classify images from the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) by using TensorFlow with Keras. DenseNet is very well-known and therefore it has been implemented and pre-trained by Keras. You are also required to load and test the pre-trained models, and compare them with your models.\n",
    "\n",
    "First of all, read the DenseNet paper. DenseNet was originally proposed in 2016 by Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger in the following paper:\n",
    "https://arxiv.org/abs/1608.06993\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">\n",
    "1. Answer a short question about DenseNet. (10 marks)\n",
    "2. Load and visualize the data.\n",
    "3. Implement your models. (30 marks)\n",
    "4. Train and evaluate your models. (25 marks)\n",
    "5. Load the pre-trained models from Keras and evaluate them. (15 marks)\n",
    "6. Analysis your results. (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Answer a short question (20 marks)\n",
    "\n",
    "Now that you know what DenseNet is all about, let's compare it to VGG.\n",
    "Both VGG and DenseNet papers describe several variations of their models that differ by their depth.\n",
    "For example, VGG16 and VGG19, DenseNet-121 and DenseNet-169 are four examples from these papers.\n",
    "\n",
    "Aside from difference in network depth, how is the architecture of DenseNet different from that of VGG? Please enter your answer in the next cell (approximately 100-200 words, both English and Chinese are acceptable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。验证了通过不断加深网络结构可以提升性能。DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and visualize the data.\n",
    "\n",
    "The data is directly loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# load the CIFAR10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#y_train=y_train.reshape(-1)\n",
    "#y_test=y_test.reshape(-1)\n",
    "# input image dimensions\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# normalize data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "# convert class vectors to binary class matrices.\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28698b13a00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---\n",
    "## 3. Implement your models (30 marks)\n",
    "\n",
    "In this task, you are required to implement DenseNet-41 and DenseNet-49 with compression and bottleneck as depicted in the original paper. DenseNet-41 and DenseNet-49 follow the architecture of DenseNet and remove several convolution layers for time efficiency. Specifically, DenseNet-41 and DenseNet-49 have a group of 4 Dense Block, each has a set of [3,6,6,3] and [4,6,8,4] mini-block including a 1x1 conv follow by a 3x3 conv. For more details, please refer to Table 1 in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the code of your DenseNet-41 model here.\n",
    "import tensorflow as tf\n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=4*num_channels, kernel_size=1)##bottleneck\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=num_channels, kernel_size=(3, 3), padding='same')\n",
    "\n",
    "        self.listLayers = [self.bn1, self.relu1, self.conv1,self.bn2, self.relu2, self.conv2]\n",
    "\n",
    "    def call(self, x):\n",
    "        y = x\n",
    "        for layer in self.listLayers.layers:\n",
    "            y = layer(y)\n",
    "        y = tf.keras.layers.concatenate([x,y], axis=-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_convs, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.listLayers = []\n",
    "        for _ in range(num_convs):\n",
    "            self.listLayers.append(ConvBlock(num_channels))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.listLayers.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TransitionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_channels, **kwargs):\n",
    "        super(TransitionBlock, self).__init__(**kwargs)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)\n",
    "        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        return self.avg_pool(x)\n",
    "    \n",
    "def block_1():\n",
    "    return tf.keras.Sequential([\n",
    "       tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
    "       tf.keras.layers.BatchNormalization(),\n",
    "       tf.keras.layers.ReLU(),\n",
    "       tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n",
    "\n",
    "def block_2():\n",
    "    net = block_1()\n",
    "    # num_channels为当前的通道数\n",
    "    num_channels, growth_rate = 64, 32\n",
    "    num_convs_in_dense_blocks = [3, 6, 6, 3]\n",
    "\n",
    "    for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "        net.add(DenseBlock(num_convs, growth_rate))\n",
    "        # 上一个稠密块的输出通道数\n",
    "        num_channels += num_convs * growth_rate\n",
    "        # 在稠密块之间添加一个转换层，使通道数量减半\n",
    "        if i != len(num_convs_in_dense_blocks) - 1:\n",
    "            num_channels //= 2  ##compression\n",
    "            net.add(TransitionBlock(num_channels))\n",
    "    return net\n",
    "\n",
    "def net1():\n",
    "    net = block_2()\n",
    "    net.add(tf.keras.layers.BatchNormalization())\n",
    "    net.add(tf.keras.layers.ReLU())\n",
    "    net.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "    net.add(tf.keras.layers.Flatten())\n",
    "    net.add(tf.keras.layers.Dense(10))\n",
    "    return net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the code of your DenseNet-49 model here.\n",
    "import tensorflow as tf\n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=4*num_channels, kernel_size=1)##bottleneck\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=num_channels, kernel_size=(3, 3), padding='same')\n",
    "\n",
    "        self.listLayers = [self.bn1, self.relu1, self.conv1,self.bn2, self.relu2, self.conv2]\n",
    "\n",
    "    def call(self, x):\n",
    "        y = x\n",
    "        for layer in self.listLayers.layers:\n",
    "            y = layer(y)\n",
    "        y = tf.keras.layers.concatenate([x,y], axis=-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_convs, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.listLayers = []\n",
    "        for _ in range(num_convs):\n",
    "            self.listLayers.append(ConvBlock(num_channels))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.listLayers.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TransitionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_channels, **kwargs):\n",
    "        super(TransitionBlock, self).__init__(**kwargs)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)\n",
    "        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        return self.avg_pool(x)\n",
    "    \n",
    "def block_1():\n",
    "    return tf.keras.Sequential([\n",
    "       tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
    "       tf.keras.layers.BatchNormalization(),\n",
    "       tf.keras.layers.ReLU(),\n",
    "       tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n",
    "\n",
    "def block_2():\n",
    "    net = block_1()\n",
    "    # num_channels为当前的通道数\n",
    "    num_channels, growth_rate = 64, 32\n",
    "    num_convs_in_dense_blocks = [4, 6, 8, 4]\n",
    "\n",
    "    for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "        net.add(DenseBlock(num_convs, growth_rate))\n",
    "        # 上一个稠密块的输出通道数\n",
    "        num_channels += num_convs * growth_rate\n",
    "        # 在稠密块之间添加一个转换层，使通道数量减半\n",
    "        if i != len(num_convs_in_dense_blocks) - 1:\n",
    "            num_channels //= 2  ##compression\n",
    "            net.add(TransitionBlock(num_channels))\n",
    "    return net\n",
    "\n",
    "def net2():\n",
    "    net = block_2()\n",
    "    net.add(tf.keras.layers.BatchNormalization())\n",
    "    net.add(tf.keras.layers.ReLU())\n",
    "    net.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "    net.add(tf.keras.layers.Flatten())\n",
    "    net.add(tf.keras.layers.Dense(10))\n",
    "    return net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train and evaluate your models. (25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train your models. (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 132s 1s/step - loss: 1.5604 - accuracy: 0.4319\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 128s 1s/step - loss: 1.1789 - accuracy: 0.5723\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 120s 1s/step - loss: 0.9631 - accuracy: 0.6566\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 120s 1s/step - loss: 0.8210 - accuracy: 0.7083\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 121s 1s/step - loss: 0.7130 - accuracy: 0.7484\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.6257 - accuracy: 0.7793\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 130s 1s/step - loss: 0.5594 - accuracy: 0.8023\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 126s 1s/step - loss: 0.4896 - accuracy: 0.8267\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 131s 1s/step - loss: 0.4369 - accuracy: 0.8455\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 131s 1s/step - loss: 0.3768 - accuracy: 0.8677\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 147s 1s/step - loss: 0.3305 - accuracy: 0.8839\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.2801 - accuracy: 0.9013\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 146s 1s/step - loss: 0.2460 - accuracy: 0.9134\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 149s 1s/step - loss: 0.2179 - accuracy: 0.9234\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1929 - accuracy: 0.9320\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1526 - accuracy: 0.9463\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.1432 - accuracy: 0.9495\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1285 - accuracy: 0.9545\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 145s 1s/step - loss: 0.1039 - accuracy: 0.9636\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 148s 1s/step - loss: 0.0981 - accuracy: 0.9649\n",
      "training accuracy:0.9648600220680237, training loss:0.09808789938688278\n"
     ]
    }
   ],
   "source": [
    "# implement your code here.\n",
    "# DenseNet-41\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 20\n",
    "net1=net1()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
    "net1.compile(optimizer, tf.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "history = net1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "print(\"training accuracy:{}, training loss:{}\".format(acc[-1], loss[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 150s 1s/step - loss: 1.5446 - accuracy: 0.4368\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 145s 1s/step - loss: 1.1712 - accuracy: 0.5784\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 143s 1s/step - loss: 0.9419 - accuracy: 0.6658\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 141s 1s/step - loss: 0.7965 - accuracy: 0.7165\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 143s 1s/step - loss: 0.6780 - accuracy: 0.7612\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 142s 1s/step - loss: 0.5995 - accuracy: 0.7894\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 141s 1s/step - loss: 0.5167 - accuracy: 0.8177\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 142s 1s/step - loss: 0.4596 - accuracy: 0.8372\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 141s 1s/step - loss: 0.3872 - accuracy: 0.8625\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 143s 1s/step - loss: 0.3522 - accuracy: 0.8740\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 144s 1s/step - loss: 0.2981 - accuracy: 0.8931\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 144s 1s/step - loss: 0.2571 - accuracy: 0.9083\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.2088 - accuracy: 0.9268\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.1859 - accuracy: 0.9342\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.1669 - accuracy: 0.9393\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.1433 - accuracy: 0.9487\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.1244 - accuracy: 0.9557\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 155s 2s/step - loss: 0.1083 - accuracy: 0.9616\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.1011 - accuracy: 0.9641\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 157s 2s/step - loss: 0.0912 - accuracy: 0.9674\n",
      "training accuracy:0.9674000144004822, training loss:0.09123262017965317\n"
     ]
    }
   ],
   "source": [
    "# DenseNet-49\n",
    "batch_size = 500\n",
    "epochs = 20\n",
    "net2=net2()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
    "net2.compile(optimizer, tf.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "history = net2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "print(\"training accuracy:{}, training loss:{}\".format(acc[-1], loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test your models. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 23s 63ms/step - loss: 1.7360 - accuracy: 0.6783\n",
      "test loss:1.7360326051712036, test accuracy:0.6783000230789185\n"
     ]
    }
   ],
   "source": [
    "# implement your code here.\n",
    "# DenseNet-41\n",
    "test_loss, test_acc = net1.evaluate(x_test, y_test)\n",
    "print(\"test loss:{}, test accuracy:{}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 13s 36ms/step - loss: 1.4860 - accuracy: 0.7016\n",
      "test loss:1.485963225364685, test accuracy:0.7016000151634216\n"
     ]
    }
   ],
   "source": [
    "# implement your code here.\n",
    "# DenseNet-49\n",
    "test_loss, test_acc = net2.evaluate(x_test, y_test)\n",
    "print(\"test loss:{}, test accuracy:{}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Load the pre-trained models from Keras and evaluate them. (15 marks)\n",
    "\n",
    "Use the pre-trained DenseNet-121 model from keras and fine-tune them by adding a few fc layers at the end. You should set the parameter weights='imagenet', rather than None, to use the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 331s 3s/step - loss: 1.8583 - accuracy: 0.3616\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 320s 3s/step - loss: 1.7315 - accuracy: 0.3838\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 1.7803 - accuracy: 0.3634\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.6139 - accuracy: 0.4224\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 1.4238 - accuracy: 0.4863\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.3094 - accuracy: 0.5338\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 1.4335 - accuracy: 0.5036\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.2659 - accuracy: 0.5576\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.1161 - accuracy: 0.6113\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.1528 - accuracy: 0.6007\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.0448 - accuracy: 0.6342\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.8887 - accuracy: 0.6842\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.8476 - accuracy: 0.7000\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.7938 - accuracy: 0.7190\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 323s 3s/step - loss: 0.8833 - accuracy: 0.6947\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 323s 3s/step - loss: 0.9705 - accuracy: 0.6697\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.3052 - accuracy: 0.5420\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 1.1086 - accuracy: 0.6141\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.8879 - accuracy: 0.6867\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 0.7860 - accuracy: 0.7204\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.7017 - accuracy: 0.7509\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.6551 - accuracy: 0.7697\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 0.5845 - accuracy: 0.7924\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.5379 - accuracy: 0.8098\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 0.4940 - accuracy: 0.8240\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 321s 3s/step - loss: 0.4523 - accuracy: 0.8399\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.4096 - accuracy: 0.8534\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.3678 - accuracy: 0.8688\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.3922 - accuracy: 0.8611\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 322s 3s/step - loss: 0.3956 - accuracy: 0.8574\n",
      "training accuracy:0.8573600053787231, training loss:0.3955577611923218\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras import Sequential,layers\n",
    "# implement your code here.\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 30\n",
    "net3 = Sequential()\n",
    "net3.add(DenseNet121(include_top=False,input_shape=(32,32,3),pooling='avg', weights='imagenet'))\n",
    "net3.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
    "net3.compile(optimizer, tf.losses.categorical_crossentropy, metrics=[\"accuracy\"])\n",
    "history = net3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "print(\"training accuracy:{}, training loss:{}\".format(acc[-1], loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 22s 70ms/step - loss: 1.0136 - accuracy: 0.6906\n",
      "test loss:1.013573169708252, test accuracy:0.6905999779701233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_acc = net3.evaluate(x_test, y_test)\n",
    "print(\"test loss:{}, test accuracy:{}\".format(test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Analysis your results. (20 marks)\n",
    "Compare the performance of your models with the following analysis. Both English and Chinese answers are acceptable.\n",
    "1. Is your implementation of DenseNet-49 better than DenseNet-41? If yes, how is the improvement? If no, try to figure the reason out based on your experiments. (10 marks)\n",
    "\n",
    "Answer:\n",
    "DenseNet-49比DenseNet-41的效果要好，DenseNet-49在测试集上的精度要高于DenseNet-41在测试集上的精度，DenseNet-49在测试集上的损失要小于DenseNet-41在测试集上的损失。\n",
    "\n",
    "2. Compare the results of your implementations with the pre-trained DenseNet-121 models from Keras. (10 marks)\n",
    "\n",
    "Answer:使用预训练的DenseNet-121的模型，训练的精度要比DenseNet-41和DenseNet-49的精度要小，测试精度和DenseNet-41和DenseNet-49的精度相差不大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
